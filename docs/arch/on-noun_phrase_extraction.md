## ğŸ” What tool does **Claimify** use for noun phrase extraction?

Claimify uses **spaCy** â€” specifically, it leverages:

* `en_core_web_sm` or `en_core_web_trf` models
* `noun_chunks` (noun phrase iterator)
* Optionally custom rule-based matchers (e.g., for terminology normalization)

Claimifyâ€™s approach is fast, reliable, and good at inline noun phrase extraction without requiring fine-tuning.

---

## ğŸ§ª Comparison with Other Python NLP Tools

### ğŸ¥‡ **spaCy** (Claimifyâ€™s choice)

* âœ… Excellent speed and tokenization quality
* âœ… Built-in noun phrase chunker (`doc.noun_chunks`)
* âœ… Easy model deployment (even transformer-backed)
* âœ… Good support for entity linking / dependency parsing
* âŒ Slightly heavier install

---

### ğŸŸ¡ **NLTK**

* âœ… Lightweight and simple
* âœ… Easy to prototype (uses regex NP grammars)
* âŒ Outdated parsing defaults (no pretrained parser)
* âŒ Less accurate chunking; requires manual POS tagging pipeline

```python
# NLTK noun phrase example (manual)
from nltk import pos_tag, word_tokenize, RegexpParser

text = "The large international consortium of researchers"
tokens = pos_tag(word_tokenize(text))
parser = RegexpParser("NP: {<DT>?<JJ>*<NN.*>+}")
result = parser.parse(tokens)
```

* Works â€” but brittle, no context-awareness

---

### ğŸŸ  **Stanza** (Stanford NLP)

* âœ… High-quality models
* âœ… Good multilingual support
* âŒ Slower than spaCy
* âŒ Less commonly used in production for fast phrase extraction

---

### ğŸŸ£ **Transformers-only (e.g. BERT + attention)**

* âœ… Theoretical best performance (e.g. noun phrase masking via attention peaks)
* âŒ Requires complex logic and training
* âŒ No off-the-shelf noun chunker

---

Note: Multilingual support for claim processing is not necessary since claims are rewritten from raw sentence input by an agentic process.
